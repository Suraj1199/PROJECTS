{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FashionMNIST_PytorchVsTensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuMwycMb33vw"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNwAIrh94tAL"
      },
      "source": [
        "# LOADING DATA - Fashion MNIST\n",
        "# DataSet class for PyTorch and tf.data for TensorFlow."
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAF6Duiu6iDe"
      },
      "source": [
        "# Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IE1NaCj6k8c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzQQqeNj7GT4"
      },
      "source": [
        "# create the transformer for put the data to the tensor\n",
        "transforms = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# load the train data - if not exists download it\n",
        "train_dataset_py = torchvision.datasets.FashionMNIST(root='/content/drive/My Drive/article/data/',\n",
        "                                             train=True, \n",
        "                                             transform=transforms,\n",
        "                                             download=True)\n",
        "\n",
        "test_dataset_py = torchvision.datasets.FashionMNIST(root='/content/drive/My Drive/article/data/',\n",
        "                                             train=False, \n",
        "                                             transform=transforms,\n",
        "                                             download=True)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqMrsAe87H6j"
      },
      "source": [
        "def imshowPytorch(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ZMCoBTZW7IDH",
        "outputId": "4bb2f45d-f137-420c-c343-29b25241f63e"
      },
      "source": [
        "# create a data loader - I will use it for the training\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset_py,\n",
        "                                           batch_size=32, \n",
        "                                           shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset_py,\n",
        "                                           batch_size=32, \n",
        "                                           shuffle=False)\n",
        "                                           \n",
        "data_iter = iter(train_loader)\n",
        "images, label = data_iter.next()\n",
        "imshowPytorch(torchvision.utils.make_grid(images[0]))\n",
        "print(label[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0LBpeby7IHB"
      },
      "source": [
        "# For PyTorch, we also have two modes of the model: train and production. \n",
        "# To put the model in the production mode, we just have to use method .eval() \n",
        "# Once the model is in the production mode, some methods will be turned off automatically, such as dropout.\n",
        "# To move it to the training mode, we have to use method .train() as train is the default mode.\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, num_of_class):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Linear(7 * 7 * 32, num_of_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CNWq0ws7IKK",
        "outputId": "eff96028-f7d4-42ad-a3bc-cb6f0424093c"
      },
      "source": [
        "modelpy = NeuralNet(10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(modelpy.parameters())\n",
        "\n",
        "modelpy"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (layer1): Sequential(\n",
              "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwEcoNm_7IWL",
        "outputId": "ab377f7f-1faf-44a0-887f-60fe77b15014"
      },
      "source": [
        "%%time\n",
        "for e in range(10):\n",
        "    # define the loss value after the epoch\n",
        "    losss = 0.0\n",
        "    number_of_sub_epoch = 0\n",
        "    \n",
        "    # loop for every training batch (one epoch)\n",
        "    for images, labels in train_loader:\n",
        "        #create the output from the network\n",
        "        out = modelpy(images)\n",
        "        # count the loss function\n",
        "        loss = criterion(out, labels)\n",
        "        # in pytorch you have assign the zero for gradient in any sub epoch\n",
        "        optim.zero_grad()\n",
        "        # count the backpropagation\n",
        "        loss.backward()\n",
        "        # learning\n",
        "        optim.step()\n",
        "        # add new value to the main loss\n",
        "        losss += loss.item()\n",
        "        number_of_sub_epoch += 1\n",
        "    print(\"step {}: loss: {}\".format(e, losss / number_of_sub_epoch))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0: loss: 0.3918272654871146\n",
            "step 1: loss: 0.27888995291193325\n",
            "step 2: loss: 0.24272490561505158\n",
            "step 3: loss: 0.21789499317804972\n",
            "step 4: loss: 0.19831469341715177\n",
            "step 5: loss: 0.18189357936282952\n",
            "step 6: loss: 0.16647196079740922\n",
            "step 7: loss: 0.1541275581859052\n",
            "step 8: loss: 0.14233158792952696\n",
            "step 9: loss: 0.13096408621097605\n",
            "CPU times: user 8min 57s, sys: 6.28 s, total: 9min 3s\n",
            "Wall time: 9min 3s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOMRmimzHteq"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "modelpy.eval()\n",
        "for images, labels in test_loader:\n",
        "    outputs = modelpy(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "print('Test Accuracy of the model on the {} test images: {}%'.format(total, 100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ax-uJzfHtmW"
      },
      "source": [
        "# save the models.\n",
        "torch.save(modelpy, \"/content/drive/My Drive/article/model.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhbyA8MgIH0y"
      },
      "source": [
        "# load the model:\n",
        "model_load_py = torch.load(\"/content/drive/My Drive/article/model.pt\")\n",
        "model_load_py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHHpY34yINzo"
      },
      "source": [
        "# check if the model was saved with the weight \n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in test_loader:\n",
        "    outputs = model_load_py(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "print('Test Accuracy of the model on the {} test images: {}%'.format(total, 100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koUGS7vB6ldt"
      },
      "source": [
        "# Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVji09qq6-nn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzUjexyM7JgM",
        "outputId": "cac6890e-88ea-45c0-e6f6-c879aff58fd8"
      },
      "source": [
        "# Load the data set\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "#split to test and train\n",
        "(train_images_tf, train_labels_tf), (test_images_tf, test_labels_tf) = fashion_mnist.load_data()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75Sc6q6B7Jir"
      },
      "source": [
        "train_images_tf = train_images_tf / 255.0\n",
        "test_images_tf = test_images_tf / 255.0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e_Q3n8l7Jk_"
      },
      "source": [
        "def imshowTensorFlow(img):\n",
        "  plt.imshow(img)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6DnztVj7Jna"
      },
      "source": [
        "imshowTensorFlow(train_images_tf[0])\n",
        "\n",
        "print(train_labels_tf[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yfsueyA7JpH"
      },
      "source": [
        "modeltf = keras.Sequential([\n",
        "    keras.layers.Conv2D(input_shape=(28,28,1), filters=16, kernel_size=5, strides=1, padding=\"same\", activation=tf.nn.relu),\n",
        "    keras.layers.BatchNormalization(), \n",
        "    keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
        "    keras.layers.Conv2D(32, kernel_size=5, strides=1, padding=\"same\", activation=tf.nn.relu),\n",
        "    keras.layers.BatchNormalization(), \n",
        "    keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djH_LEhiFR0g"
      },
      "source": [
        "modeltf.compile(optimizer=keras.optimizers.Adam(), \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "modeltf.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRGv1OzTG2iY"
      },
      "source": [
        "# Because Fashion MNIST in TensorFlow is an array with only two dimensions, \n",
        "# we have to add the number of channels (in our case itâ€™s just one):\n",
        "\n",
        "train_images_tf = train_images_tf.reshape(train_images_tf.shape[0], \n",
        "                                          train_images_tf.shape[1],\n",
        "                                          train_images_tf.shape[2], 1)\n",
        " %%time\n",
        "modeltf.fit(train_images_tf, train_labels_tf, epochs=10, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgRbn4tKFR_5"
      },
      "source": [
        "test_images_tf = test_images_tf.reshape(test_images_tf.shape[0],\n",
        "                                        test_images_tf.shape[1],\n",
        "                                        test_images_tf.shape[2], 1)\n",
        "predictions = modeltf.predict(test_images_tf)\n",
        "correct = 0\n",
        "for i, pred in enumerate(predictions):\n",
        "  if np.argmax(pred) == test_labels_tf[i]:\n",
        "    correct += 1\n",
        "print('Test Accuracy of the model on the {} test images: {}%'.format(test_images_tf.shape[0],\n",
        "                                                                     100 * correct/test_images_tf.shape[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74REe6bOFSCV"
      },
      "source": [
        "# another way of inference\n",
        "test_loss, test_acc = modeltf.evaluate(test_images_tf, test_labels_tf)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyJSFjgvIV6o"
      },
      "source": [
        "# save the model \n",
        "modeltf.save('modeltf.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx_5-zmaIbr9"
      },
      "source": [
        "# load the model\n",
        "model_load_tf = tf.keras.models.load_model('modeltf.h5')\n",
        "model_load_tf.summary()\n",
        "test_loss, test_acc = model_load_tf.evaluate(test_images_tf, test_labels_tf)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Knk0m8YIomj"
      },
      "source": [
        "# In summary\n",
        "<hr>\n",
        "\n",
        "**Tensorflow**\n",
        ">Pros:\n",
        "* Simple built-in high level API\n",
        "* Tensorboard (easy to use visualisation tool)\n",
        "* Simple serving method on production\n",
        "* Very good documentation\n",
        "* Easy mobile support\n",
        "\n",
        ">Cons:\n",
        "* Static graph\n",
        "* Debugging method\n",
        "* Hard to make quick changes\n",
        "<hr>\n",
        "\n",
        "**Pytorch**\n",
        ">Pros:\n",
        "* Python-like coding\n",
        "* Dynamic graph\n",
        "* Easy & quick editing\n",
        "* Very good documentation available\n",
        "* Plenty of projects out there which use Pytorch\n",
        "\n",
        ">Cons:\n",
        "* Third party needed for visualisation\n",
        "* API knowledge needed in Python to move to production\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDctpY2dJSE8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}