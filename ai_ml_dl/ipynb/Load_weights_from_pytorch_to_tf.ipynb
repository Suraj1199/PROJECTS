{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YuMwycMb33vw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# LOADING DATA - Fashion MNIST\n",
    "# DataSet class for PyTorch and tf.data for TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAF6Duiu6iDe"
   },
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0IE1NaCj6k8c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dzQQqeNj7GT4"
   },
   "outputs": [],
   "source": [
    "# create the transformer for put the data to the tensor\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# load the train data - if not exists download it\n",
    "train_dataset_py = torchvision.datasets.FashionMNIST(root='./sample_data',\n",
    "                                             train=True,\n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset_py = torchvision.datasets.FashionMNIST(root='./sample_data',\n",
    "                                             train=False, \n",
    "                                             transform=transform,\n",
    "                                             download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GqMrsAe87H6j"
   },
   "outputs": [],
   "source": [
    "def imshowPytorch(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "ZMCoBTZW7IDH",
    "outputId": "17f00e77-bc0e-40d3-a832-17a7a475fb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 28, 28)\n",
      "tensor(9)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a data loader - I will use it for the training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset_py,\n",
    "                                           batch_size=32,\n",
    "                                           shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset_py,\n",
    "                                           batch_size=32,\n",
    "                                           shuffle=False)\n",
    "                                           \n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "images, labels = next(data_iter)\n",
    "print(images.numpy().shape)\n",
    "imshowPytorch(torchvision.utils.make_grid(images[0]))\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "n0LBpeby7IHB"
   },
   "outputs": [],
   "source": [
    "# For PyTorch, we also have two modes of the model: train and production. \n",
    "# To put the model in the production mode, we just have to use method .eval() \n",
    "# Once the model is in the production mode, some methods will be turned off automatically, such as dropout.\n",
    "# To move it to the training mode, we have to use method .train() as train is the default mode.\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, num_of_class):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # self.layer2 = nn.Sequential(\n",
    "        #     nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "        #     # nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # self.fc = nn.Linear(7 * 7 * 32, num_of_class)\n",
    "        \n",
    "        self.fc = nn.Linear(9216, num_of_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        #         # out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CNWq0ws7IKK",
    "outputId": "5532e9d1-d66b-4c85-b71d-8e14e8c42d55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=9216, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_py = NeuralNet(10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model_py.parameters())\n",
    "\n",
    "model_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CwEcoNm_7IWL",
    "outputId": "0366eff2-ebe7-41f9-b80b-d2f986b10bd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss: 0.44893012918829917\n",
      "CPU times: user 49.5 s, sys: 484 ms, total: 50 s\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for e in range(1):\n",
    "    # define the loss value after the epoch\n",
    "    losss = 0.0\n",
    "    number_of_sub_epoch = 0\n",
    "    \n",
    "    # loop for every training batch (one epoch)\n",
    "    for images, labels in train_loader:\n",
    "        #create the output from the network\n",
    "        out = model_py(images)\n",
    "        # count the loss function\n",
    "        loss = criterion(out, labels)\n",
    "        # in pytorch you have assign the zero for gradient in any sub epoch\n",
    "        optim.zero_grad()\n",
    "        # count the backpropagation\n",
    "        loss.backward()\n",
    "        # learning\n",
    "        optim.step()\n",
    "        # add new value to the main loss\n",
    "        losss += loss.item()\n",
    "        number_of_sub_epoch += 1\n",
    "    print(\"step {}: loss: {}\".format(e, losss / number_of_sub_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pOMRmimzHteq",
    "outputId": "2ce3ab40-b851-4621-a41d-918d38caf9d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 87.48999786376953%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model_py.eval()\n",
    "for images, labels in test_loader:\n",
    "    outputs = model_py(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "print('Test Accuracy of the model on the {} test images: {}%'.format(total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "8ax-uJzfHtmW"
   },
   "outputs": [],
   "source": [
    "# save the models.\n",
    "torch.save(model_py, \"model_py.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Loading the saved pytorch model**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhbyA8MgIH0y",
    "outputId": "487dee07-ab8a-44e9-ea66-a057ca5b1297"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=9216, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model:\n",
    "model_load_py = torch.load(\"model_py.pt\")\n",
    "model_load_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3k_Y4S3chFX",
    "outputId": "9919bf42-7837-4be3-dc36-5e29fcddda61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 24, 24]             416\n",
      "              ReLU-2           [-1, 16, 24, 24]               0\n",
      "       BatchNorm2d-3           [-1, 16, 24, 24]              32\n",
      "            Linear-4                   [-1, 10]          92,170\n",
      "================================================================\n",
      "Total params: 92,618\n",
      "Trainable params: 92,618\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.21\n",
      "Params size (MB): 0.35\n",
      "Estimated Total Size (MB): 0.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# keras like summary\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model_load_py, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OHHpY34yINzo",
    "outputId": "0b62ac6f-6dd1-4491-9380-a118366e740a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 87.48999786376953%\n"
     ]
    }
   ],
   "source": [
    "# check if the model was saved with the weight \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    outputs = model_load_py(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "print('Test Accuracy of the model on the {} test images: {}%'.format(total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-liuo1w2NtZi",
    "outputId": "002ed169-c695-4879-dd73-31c61f2b07da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1, 5, 5) [ 0.1565587   0.12268871  0.12622112 -0.22293016  0.1205987 ]\n",
      "(16,) [ 0.02704382 -0.14164135 -0.227082    0.03655862 -0.09029163]\n",
      "(16,) [0.35765034 0.85456157 0.73478514 0.50098115 0.5532688 ]\n",
      "(16,) [-0.02144467  0.00027201  0.01555244  0.08424513  0.05354267]\n",
      "(16,) [1.9148977e-01 9.8308428e-06 7.1580798e-05 1.2939961e-02 1.3746850e-02]\n",
      "(16,) [2.5555780e-02 4.3356511e-07 6.1809624e-06 8.0470799e-04 1.7845844e-03]\n",
      "(10, 9216) [-0.02324438 -0.00191114 -0.00253256 -0.0063062  -0.01364839]\n",
      "(10,) [ 0.02073159  0.00741996 -0.00192046  0.00374859 -0.02121427]\n"
     ]
    }
   ],
   "source": [
    "# load weights from pytorch model\n",
    "\n",
    "# #CONV2D\n",
    "conv2d = model_load_py.layer1[0].state_dict()\n",
    "weights0 = conv2d['weight'].numpy()\n",
    "print(weights0.shape, weights0[0, 0, 0, :5])\n",
    "bias0 = conv2d['bias'].numpy()\n",
    "print(bias0.shape, bias0[:5])\n",
    "\n",
    "# BATCH NORM\n",
    "batch_norm = model_load_py.layer1[2].state_dict()\n",
    "weight0 = batch_norm['weight'].numpy()\n",
    "bias0 = batch_norm['bias'].numpy()\n",
    "rmean = batch_norm['running_mean'].numpy()\n",
    "rvar = batch_norm['running_var'].numpy()\n",
    "print(weight0.shape, weight0[:5])\n",
    "print(bias0.shape, bias0[:5])\n",
    "print(rmean.shape, rmean[:5])\n",
    "print(rvar.shape, rvar[:5])\n",
    "\n",
    "# LINEAR\n",
    "fc = model_load_py.fc.state_dict()\n",
    "weights = fc['weight'].numpy()\n",
    "print(weights.shape, weights[0, :5])\n",
    "bias = fc['bias'].numpy()\n",
    "print(bias.shape, bias[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koUGS7vB6ldt"
   },
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "iVji09qq6-nn"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "ZzUjexyM7JgM"
   },
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "#split to test and train\n",
    "(train_images_tf, train_labels_tf), (test_images_tf, test_labels_tf) = fashion_mnist.load_data()\n",
    "\n",
    "train_images_tf = train_images_tf / 255.0\n",
    "test_images_tf = test_images_tf / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whP-XxY_M02C",
    "outputId": "656fb2b7-170e-4a4e-c0a3-711157d6135e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape before (60000, 28, 28)\n",
      "Test Shape before (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Shape before\", train_images_tf.shape)\n",
    "print(\"Test Shape before\", test_images_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "9whqLuGVbFZx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape after (60000, 28, 28, 1)\n",
      "Test Shape after (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_images_tf = train_images_tf[:, :, :, np.newaxis]\n",
    "print(\"Train Shape after\", train_images_tf.shape)\n",
    "test_images_tf = test_images_tf[:, :, :, np.newaxis]\n",
    "print(\"Test Shape after\", test_images_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "u6DnztVj7Jna",
    "outputId": "206084d3-f63f-4b17-af0b-46678469a21d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f13040a1ca0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images_tf[0].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "_yfsueyA7JpH"
   },
   "outputs": [],
   "source": [
    "model_tf = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28, 1), batch_size=32),\n",
    "    keras.layers.Conv2D(filters=16, kernel_size=5, strides=1, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(), \n",
    "    # keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "    # keras.layers.Conv2D(filters=32, kernel_size=5, strides=1, padding=\"same\", activation=tf.nn.relu),\n",
    "    # keras.layers.BatchNormalization(), \n",
    "    # keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "    keras.layers.Permute((3, 1, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djH_LEhiFR0g",
    "outputId": "45c46284-9265-4512-9298-79b4513eecfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (32, 24, 24, 16)          416       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (32, 24, 24, 16)          64        \n",
      "_________________________________________________________________\n",
      "permute_5 (Permute)          (32, 16, 24, 24)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (32, 9216)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (32, 10)                  92170     \n",
      "=================================================================\n",
      "Total params: 92,650\n",
      "Trainable params: 92,618\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_tf.compile(optimizer=keras.optimizers.Adam(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XjADu3JOdMr",
    "outputId": "04ba58df-a643-40e4-848c-883138df3ea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 1, 16) [ 0.00230907 -0.05596308  0.06874196  0.08860019 -0.11029251]\n",
      "(16,) [0. 0. 0. 0. 0.]\n",
      "(16,) [1. 1. 1. 1. 1.]\n",
      "(16,) [0. 0. 0. 0. 0.]\n",
      "(16,) [0. 0. 0. 0. 0.]\n",
      "(16,) [1. 1. 1. 1. 1.]\n",
      "(9216, 10) [-0.01712917  0.02042163 -0.02043371  0.02418322 -0.00282468]\n",
      "(10,) [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# get the current weights of tensorflow model\n",
    "\n",
    "# # CONV2D\n",
    "w0, b0 = model_tf.layers[0].get_weights()\n",
    "print(w0.shape, w0[0, 0, 0, :5])\n",
    "print(b0.shape, b0[:5])\n",
    "\n",
    "# BATCH NORM\n",
    "w1, b1, mu, var = model_tf.layers[1].get_weights()\n",
    "print(w1.shape, w1[:5])\n",
    "print(b1.shape, b1[:5])\n",
    "print(mu.shape, mu[:5])\n",
    "print(var.shape, var[:5])\n",
    "\n",
    "# lINEAR\n",
    "w, b = model_tf.layers[4].get_weights()\n",
    "print(w.shape, w[0,:5])\n",
    "print(b.shape, b[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dUUd8MKKc9xJ",
    "outputId": "cac77964-38da-4698-b81e-ecaf7047873d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 10.71%\n"
     ]
    }
   ],
   "source": [
    "predictions = model_tf.predict(test_images_tf)\n",
    "correct = 0\n",
    "for i, pred in enumerate(predictions):\n",
    "    if np.argmax(pred) == test_labels_tf[i]:\n",
    "        correct += 1\n",
    "print('Test Accuracy of the model on the {} test images: {}%'.format(test_images_tf.shape[0],\n",
    "                                                                     100 * correct/test_images_tf.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqVNN_GGhNTI",
    "outputId": "a5b095cf-a5ee-4edd-d0a8-cac3a17c02fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 8ms/step - loss: 2.3198 - accuracy: 0.1069\n",
      "accuracy 10.692891478538513%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model_tf.evaluate(test_images_tf, test_labels_tf)\n",
    "print(f\"accuracy {acc * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fN2VkOliOdfi",
    "outputId": "47a6cc8f-cc17-4824-839f-d511915500ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 1, 16) [ 0.1565587  -0.14232314  0.03972157  0.12053496  0.01569638]\n",
      "(16,) [-0.02144467  0.00027201  0.01555244  0.08424513  0.05354267]\n",
      "(16,) [0.35765034 0.85456157 0.73478514 0.50098115 0.5532688 ]\n",
      "(16,) [-0.02144467  0.00027201  0.01555244  0.08424513  0.05354267]\n",
      "(16,) [1.9148977e-01 9.8308428e-06 7.1580798e-05 1.2939961e-02 1.3746850e-02]\n",
      "(16,) [2.5555780e-02 4.3356511e-07 6.1809624e-06 8.0470799e-04 1.7845844e-03]\n",
      "(9216, 10) [-0.02324438  0.00205207  0.00207317  0.01123369  0.00536022]\n",
      "(10,) [ 0.02073159  0.00741996 -0.00192046  0.00374859 -0.02121427]\n"
     ]
    }
   ],
   "source": [
    "# pass loaded weights to tensorflow model\n",
    "\n",
    "# #CONV2D\n",
    "model_tf.layers[0].set_weights([weights0.transpose(2, 3, 1, 0), bias0.T])\n",
    "w0, b0 = model_tf.layers[0].get_weights()\n",
    "print(w0.shape, w0[0, 0, 0, :5])\n",
    "print(b0.shape, b0[:5])\n",
    "\n",
    "#BATCH NORM\n",
    "model_tf.layers[1].set_weights([weight0.T, bias0.T, rmean.T, rvar.T])\n",
    "w1, b1, mu, var = model_tf.layers[1].get_weights()\n",
    "print(w1.shape, w1[:5])\n",
    "print(b1.shape, b1[:5])\n",
    "print(mu.shape, mu[:5])\n",
    "print(var.shape, var[:5])\n",
    "\n",
    "#DENSE\n",
    "model_tf.layers[4].set_weights([weights.T, bias.T])\n",
    "w, b = model_tf.layers[4].get_weights()\n",
    "print(w.shape, w[0,:5])\n",
    "print(b.shape, b[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2KQIxSUdL8A",
    "outputId": "c239e038-d284-4c57-d47f-4db514a90458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 61.34%\n"
     ]
    }
   ],
   "source": [
    "predictions = model_tf.predict(test_images_tf)\n",
    "correct = 0\n",
    "for i, pred in enumerate(predictions):\n",
    "    if np.argmax(pred) == test_labels_tf[i]:\n",
    "        correct += 1\n",
    "print('Test Accuracy of the model on the {} test images: {}%'.format(test_images_tf.shape[0],\n",
    "                                                                     100 * correct/test_images_tf.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1UsKFm28gC8E",
    "outputId": "f27e4318-8f72-4a11-f12a-d86760b62a80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5989 - accuracy: 0.6124\n",
      "accuracy 61.24201416969299%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model_tf.evaluate(test_images_tf, test_labels_tf)\n",
    "print(f\"accuracy {acc * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzHLXDkNfDtS"
   },
   "outputs": [],
   "source": [
    "# save the model \n",
    "model_tf.save('model_tf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7_4e6dlfds7",
    "outputId": "9bb20051-31dd-4ff7-88ab-c6b15202ba10"
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "model_load_tf = tf.keras.models.load_model('model_tf.h5')\n",
    "model_load_tf.summary()\n",
    "test_loss, test_acc = model_load_tf.evaluate(test_images_tf, test_labels_tf)\n",
    "print('Test accuracy:', test_acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt0IQuFFfdxd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFIxzmYHfEw3"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrzI8ulYfdze"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBlg8kTswq64"
   },
   "outputs": [],
   "source": [
    "# weights and bias of layers from pytorch loaded model\n",
    "py_weights_1 = np.array(model_load_py.layer1[0].weight.data).T\n",
    "py_bias_1 = np.array(model_load_py.layer1[0].bias.data).T\n",
    "\n",
    "# py_weights_2 = np.array(model_load_py.layer1[1].weight.data).T\n",
    "# py_bias_2 = np.array(model_load_py.layer1[1].bias.data).T\n",
    "\n",
    "py_weights_5 = np.array(model_load_py.layer2[0].weight.data).T\n",
    "py_bias_5 = np.array(model_load_py.layer2[0].bias.data).T\n",
    "\n",
    "# py_weights_6 = np.array(model_load_py.layer2[1].weight.data)\n",
    "# py_bias_6 = np.array(model_load_py.layer2[1].bias.data)\n",
    "\n",
    "py_weights_9 = np.array(model_load_py.fc.weight.data).T\n",
    "py_bias_9 = np.array(model_load_py.fc.bias.data).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5MU1XEQyNS7"
   },
   "outputs": [],
   "source": [
    "modeltf.layers[0].set_weights([py_weights_1, py_bias_1])\n",
    "# modeltf.layers[1].set_weights([py_weights_2, py_bias_2, np.zeros(py_weights_2.shape), np.ones(py_weights_2.shape)])\n",
    "modeltf.layers[2].set_weights([py_weights_5, py_bias_5])\n",
    "# modeltf.layers[4].set_weights([py_weights_6, py_bias_6, np.zeros(py_weights_6.shape), np.ones(py_weights_6.shape)])\n",
    "modeltf.layers[5].set_weights([py_weights_9, py_bias_9])\n",
    "\n",
    "# modeltf.layers[0].get_weights()\n",
    "# modeltf.layers[0].set_weights([w, b])\n",
    "# weights, bias = modeltf.layers[0].get_weights()\n",
    "# weights_tf1, bias_tf1, _, _= np.array(modeltf.layers[1].get_weights())\n",
    "# weights_tf1 = weights_py1\n",
    "# print(weights_tf1)\n",
    "# modeltf.layers[1].get_weights()[0]\n",
    "# print(w.shape)\n",
    "# print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sNayRyrI2hj"
   },
   "source": [
    "### DO NOT TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ueOE6zHEWbZ4",
    "outputId": "045e4d7a-1d88-4924-c09c-63e9aae08217"
   },
   "outputs": [],
   "source": [
    "predictions = model_tf.predict(test_images_tf)\n",
    "correct = 0\n",
    "for i, pred in enumerate(predictions):\n",
    "  if np.argmax(pred) == test_labels_tf[i]:\n",
    "    correct += 1\n",
    "print('Test Accuracy of the model on the {} test images: {}%'.format(test_images_tf.shape[0],\n",
    "                                                                     100 * correct/test_images_tf.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRGv1OzTG2iY",
    "outputId": "411979a7-df6a-4bf6-9d09-9ffc306bca24"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Because Fashion MNIST in TensorFlow is an array with only two dimensions, \n",
    "# we have to add the number of channels (in our case itâ€™s just one):\n",
    "\n",
    "train_images_tf = train_images_tf.reshape(1, train_images_tf.shape[0], \n",
    "                                          train_images_tf.shape[1],\n",
    "                                          train_images_tf.shape[2])\n",
    "\n",
    "modeltf.fit(train_images_tf, train_labels_tf, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "74REe6bOFSCV",
    "outputId": "30cb935d-4fcc-4c7f-e106-0784f98c34e9"
   },
   "outputs": [],
   "source": [
    "# another way of inference\n",
    "test_loss, test_acc = modeltf.evaluate(test_images_tf, test_labels_tf)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9tVaM71JHur"
   },
   "source": [
    "## CONTINUE FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyJSFjgvIV6o"
   },
   "outputs": [],
   "source": [
    "# save the model \n",
    "modeltf.save('modeltf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qx_5-zmaIbr9",
    "outputId": "9baec760-b97f-4a4b-d7f1-10923e75411e"
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "model_load_tf = tf.keras.models.load_model('modeltf.h5')\n",
    "model_load_tf.summary()\n",
    "test_loss, test_acc = model_load_tf.evaluate(test_images_tf, test_labels_tf)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Knk0m8YIomj"
   },
   "source": [
    "# In summary\n",
    "<hr>\n",
    "\n",
    "**Tensorflow**\n",
    ">Pros:\n",
    "* Simple built-in high level API\n",
    "* Tensorboard (easy to use visualisation tool)\n",
    "* Simple serving method on production\n",
    "* Very good documentation\n",
    "* Easy mobile support\n",
    "\n",
    ">Cons:\n",
    "* Static graph\n",
    "* Debugging method\n",
    "* Hard to make quick changes\n",
    "<hr>\n",
    "\n",
    "**Pytorch**\n",
    ">Pros:\n",
    "* Python-like coding\n",
    "* Dynamic graph\n",
    "* Easy & quick editing\n",
    "* Very good documentation available\n",
    "* Plenty of projects out there which use Pytorch\n",
    "\n",
    ">Cons:\n",
    "* Third party needed for visualisation\n",
    "* API knowledge needed in Python to move to production\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsS5EKa2cL_r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFPL0c6zx9WW"
   },
   "source": [
    "<pre>\n",
    "You can check shapes of all weights of all keras layers quite simply:\n",
    "\n",
    "for layer in model.layers:\n",
    "    print([tensor.shape for tensor in layer.get_weights()])\n",
    "This would give you shapes of all weights (including biases), so you can prepare loaded numpy weights accordingly.\n",
    "\n",
    "To set them, do something similar:\n",
    "\n",
    "for torch_weight, layer in zip(model.layers, torch_weights):\n",
    "    layer.set_weights(torch_weight)\n",
    "where torch_weights should be a list containing lists of np.array which you would have to load.\n",
    "\n",
    "Usually each element of torch_weights would contain one np.array for weights and one for bias.\n",
    "\n",
    "Remember shapes received from print have to be exactly the same as the ones you put in set_weights.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2e3zq5rIcMCC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6cu51sucMEJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcqLXo4lcJvr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Load weights from pytorch to tf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
